{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fwk.neural_nets as nn\n",
    "import fwk.optimizers as optim\n",
    "import fwk.data as data\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Disable auto_grad\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Global variables\n",
    "N_EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, Y_train_hot = data.generate_data()\n",
    "X_test, Y_test, Y_test_hot = data.generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.f = nn.Sequential(OrderedDict(\n",
    "            {\n",
    "                'linear 1': nn.Linear(2, 25),\n",
    "                'relu 1': nn.Tanh(),\n",
    "                'linear 2': nn.Linear(25, 25),\n",
    "                'relu 2': nn.Tanh(),\n",
    "                'linear 3': nn.Linear(25, 25),\n",
    "                'relu 3': nn.Tanh(),\n",
    "                'linear 4': nn.Linear(25, 2),\n",
    "                'last act': nn.Tanh(),\n",
    "            }\n",
    "        )\n",
    "        )\n",
    "\n",
    "    def backward(self, d_loss):\n",
    "        return self.f.backward(d_loss)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.f.forward(x)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        return self.f.zero_grad()\n",
    "    \n",
    "    def param(self):\n",
    "        return self.f.param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, Y_train_hot, model, optimizer, criterion, n_epochs=100, batch_size=10, verbose=False):\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = 0\n",
    "        \n",
    "        for batch in range(0, len(X_train), batch_size):\n",
    "\n",
    "            # Build batch \n",
    "            X_batch = X_train.narrow(0, batch, batch_size)\n",
    "            Y_batch = Y_train_hot.narrow(0, batch, batch_size)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model.forward(X_batch) # Model prediction\n",
    "            loss += criterion.forward(output, Y_batch)\n",
    "            \n",
    "            # Backward pass\n",
    "            model.zero_grad()\n",
    "            grad = criterion.backward()\n",
    "            model.backward(grad) # Compute the gradients with predictions\n",
    "            optimizer.step() # Optimize parameters\n",
    "        if verbose:\n",
    "            print(\"Epoch {}: Total loss={}\".format(epoch, loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomNet()\n",
    "criterion = nn.MSE()\n",
    "optimizer = optim.SGD(model.param())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss=592.1658325195312\n",
      "Epoch 1: Total loss=331.9757995605469\n",
      "Epoch 2: Total loss=277.70635986328125\n",
      "Epoch 3: Total loss=250.56549072265625\n",
      "Epoch 4: Total loss=213.40252685546875\n",
      "Epoch 5: Total loss=189.43829345703125\n",
      "Epoch 6: Total loss=177.2915496826172\n",
      "Epoch 7: Total loss=157.93325805664062\n",
      "Epoch 8: Total loss=137.73887634277344\n",
      "Epoch 9: Total loss=122.0956039428711\n",
      "Epoch 10: Total loss=115.52684020996094\n",
      "Epoch 11: Total loss=111.31877136230469\n",
      "Epoch 12: Total loss=108.20585632324219\n",
      "Epoch 13: Total loss=105.69670867919922\n",
      "Epoch 14: Total loss=103.56249237060547\n",
      "Epoch 15: Total loss=101.6838150024414\n",
      "Epoch 16: Total loss=99.9904556274414\n",
      "Epoch 17: Total loss=98.43766021728516\n",
      "Epoch 18: Total loss=96.99530029296875\n",
      "Epoch 19: Total loss=95.64200592041016\n",
      "Epoch 20: Total loss=94.36170196533203\n",
      "Epoch 21: Total loss=93.1415023803711\n",
      "Epoch 22: Total loss=91.97171783447266\n",
      "Epoch 23: Total loss=90.84466552734375\n",
      "Epoch 24: Total loss=89.75479125976562\n",
      "Epoch 25: Total loss=88.69805908203125\n",
      "Epoch 26: Total loss=87.67195129394531\n",
      "Epoch 27: Total loss=86.67488861083984\n",
      "Epoch 28: Total loss=85.70628356933594\n",
      "Epoch 29: Total loss=84.76594543457031\n",
      "Epoch 30: Total loss=83.85408020019531\n",
      "Epoch 31: Total loss=82.97057342529297\n",
      "Epoch 32: Total loss=82.11538696289062\n",
      "Epoch 33: Total loss=81.28800964355469\n",
      "Epoch 34: Total loss=80.48780822753906\n",
      "Epoch 35: Total loss=79.71408081054688\n",
      "Epoch 36: Total loss=78.96598052978516\n",
      "Epoch 37: Total loss=78.24254608154297\n",
      "Epoch 38: Total loss=77.54285430908203\n",
      "Epoch 39: Total loss=76.865966796875\n",
      "Epoch 40: Total loss=76.21105194091797\n",
      "Epoch 41: Total loss=75.57708740234375\n",
      "Epoch 42: Total loss=74.96322631835938\n",
      "Epoch 43: Total loss=74.36872863769531\n",
      "Epoch 44: Total loss=73.79280090332031\n",
      "Epoch 45: Total loss=73.23486328125\n",
      "Epoch 46: Total loss=72.69426727294922\n",
      "Epoch 47: Total loss=72.17039489746094\n",
      "Epoch 48: Total loss=71.66291809082031\n",
      "Epoch 49: Total loss=71.17111206054688\n",
      "Epoch 50: Total loss=70.69459533691406\n",
      "Epoch 51: Total loss=70.23291015625\n",
      "Epoch 52: Total loss=69.7855453491211\n",
      "Epoch 53: Total loss=69.35212707519531\n",
      "Epoch 54: Total loss=68.93230438232422\n",
      "Epoch 55: Total loss=68.52535247802734\n",
      "Epoch 56: Total loss=68.13111114501953\n",
      "Epoch 57: Total loss=67.74898529052734\n",
      "Epoch 58: Total loss=67.37850189208984\n",
      "Epoch 59: Total loss=67.01921081542969\n",
      "Epoch 60: Total loss=66.67074584960938\n",
      "Epoch 61: Total loss=66.33258819580078\n",
      "Epoch 62: Total loss=66.0042724609375\n",
      "Epoch 63: Total loss=65.68539428710938\n",
      "Epoch 64: Total loss=65.37548828125\n",
      "Epoch 65: Total loss=65.07421875\n",
      "Epoch 66: Total loss=64.78108978271484\n",
      "Epoch 67: Total loss=64.49569702148438\n",
      "Epoch 68: Total loss=64.21772766113281\n",
      "Epoch 69: Total loss=63.9467658996582\n",
      "Epoch 70: Total loss=63.682525634765625\n",
      "Epoch 71: Total loss=63.424564361572266\n",
      "Epoch 72: Total loss=63.17271423339844\n",
      "Epoch 73: Total loss=62.926513671875\n",
      "Epoch 74: Total loss=62.685787200927734\n",
      "Epoch 75: Total loss=62.450294494628906\n",
      "Epoch 76: Total loss=62.219627380371094\n",
      "Epoch 77: Total loss=61.99374008178711\n",
      "Epoch 78: Total loss=61.77225112915039\n",
      "Epoch 79: Total loss=61.55500030517578\n",
      "Epoch 80: Total loss=61.34178924560547\n",
      "Epoch 81: Total loss=61.13246536254883\n",
      "Epoch 82: Total loss=60.926841735839844\n",
      "Epoch 83: Total loss=60.724700927734375\n",
      "Epoch 84: Total loss=60.52593231201172\n",
      "Epoch 85: Total loss=60.33040237426758\n",
      "Epoch 86: Total loss=60.1379280090332\n",
      "Epoch 87: Total loss=59.94830322265625\n",
      "Epoch 88: Total loss=59.76164245605469\n",
      "Epoch 89: Total loss=59.577659606933594\n",
      "Epoch 90: Total loss=59.396270751953125\n",
      "Epoch 91: Total loss=59.21735763549805\n",
      "Epoch 92: Total loss=59.040889739990234\n",
      "Epoch 93: Total loss=58.866676330566406\n",
      "Epoch 94: Total loss=58.694801330566406\n",
      "Epoch 95: Total loss=58.524993896484375\n",
      "Epoch 96: Total loss=58.35725402832031\n",
      "Epoch 97: Total loss=58.19160461425781\n",
      "Epoch 98: Total loss=58.02781295776367\n",
      "Epoch 99: Total loss=57.86594772338867\n"
     ]
    }
   ],
   "source": [
    "train(X_train, Y_train_hot, model, optimizer, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, X_test, Y_test):\n",
    "    pred = model.forward(X_test).max(1)[1]\n",
    "    errors = (Y_test == pred).sum().item()\n",
    "    return errors / len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc: 0.981\n",
      "Train acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "print(\"Test acc:\", accuracy(model, X_test, Y_test))\n",
    "print(\"Train acc:\", accuracy(model, X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
