\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment


\begin{document}
\title{ Mini Deep Learning Framework \\ Deep Learning (EE559) - Project 2 }

\author{
  Rayane Laraki, Pierre Schutz\\
  \textit{EPFL, Switzerland}
}


\maketitle

\section{Introduction}

The goal of this project is to design a mini-framework to build deep learning models in python. 
It has to built using only PyTorch tensor operations and Python math library, without autograd. 

This report presents the architecture of this small library, with some test and performance analysis.

\section{Functionalities}

The framework provide the following tools:
\begin{itemize}
    \item Linear fully connected layer module to build sequential networks
    \item ReLu and Tanh activation functions
    \item Stochastic gradient descent optimizer.
    \item Mean Squared Error loss function
\end{itemize}


\section{Architecture}

The framework is divided into two main categories: neural network package and optimizers. The neural network modules is similar to the PyTorch one. It contains all components you need inside a neural network for it to work and train properly, such as feed-forward fully connected layers, activation functions, and loss function. It also contains the sequential module which encapsulate a list of layers to make a full network. The optimizer package the model optimizers such as stochastic gradient descent algorithm. We are going to details each component of our framework though their usage and implementation.

\subsection{Module}

\texttt{Module} is an abstract class than represents a neural network module. It is the parent class of all other modules and determine the main methods than should be defined. 

\texttt{forward}: Define module behavior during the forward pass of a neural network training.

\texttt{backward}: Define module behavior during the backward pass of a neural network training. 

\textt{param}: Modules should return their parameters and gradients so that it can be update by the optimizer. In general it return an empty list (for parameter-less modules)

\texttt{zero\_grad}: This method should reset the gradient value of a module when called. In general it is just ignored (for parameter-less modules).

\subsection{Rectified Linear Unit activation (ReLU)}

This activation function module compute the output of ReLU function given the input parameter. It helps bind the output values within a given range a is usually used after a layer of neural network. The module stores an input parameter to keep the input of forward pass during backward computation. The ReLU function and derivative are defined in the methods called \texttt{\_\_sigma} and \texttt{\_\_dsigma} respectively.

\texttt{forward}: During the forward pass, the \texttt{ReLU} module simply compute the ReLU function of the input:

\begin{equation}
   ReLU(x) = max(0, x) 
\end{equation}

\texttt{backward}: During the backward pass, the \texttt{ReLU} module return the derivative of the ReLU function times the given gradient. It therefore propagate the gradient computation for that layer. 

\begin{equation}
    ReLU'(x) = \left\{
    \begin{array}{lr}
        1 &  \textrm{if }x > 0\\
        0 & \textrm{otherwise}
    \end{array}\right\}
    
\end{equation}


\subsection{Hyperbolic Tangent activation (Tanh)}

Similarly as ReLU, this activation function module help to bind the output to predefined range for a given input. This activation function use the hyperbolic tangent function to bind input to $[-1,1]$ range.

The module is similar in all means to the ReLU. Only the function change:


\begin{equation}
    Tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}
\end{equation}



\begin{equation}
    Tanh'(x) = 1 - Tanh(x)^2
\end{equation}

\subsection{Linear}

This module represents a fully connected linear layer of a neural network. It stores a weights matrix corresponding to the input of output size of the layer as well as a bias vector. This module also stores the gradients of those parameters. 

\texttt{\_\_init\_\_(input\_size, output\_size}: The init function create the weight, bias parameters and their gradients. It also create an input parameter that will store the layer input between forward and backward pass. The weight and bias are initialized with normal distribution. 

\texttt{forward}: The forward pass computes the return of the fully connected layer by multiplying the input my the weights and adding the bias term. The input vector is a matrix to enable mini batches and process multiple input in the same pass.

\texttt{backward}: The backward pass computes the gradients with respect to all parameters and update the parameter gradient values. 

\texttt{param}: Return a list of pair containing the weight, bias and for each of them their gradients for the optimizer. 

\texttt{zero\_grad}: Reset the weight and bias gradient vectors to 0.

\subsection{Sequential}

This module represents a group of module assembled together sequentially. It will contain multiple pairs of linear layers and activation functions. The sequential module can represent a fully neural network. We chose to store the child modules in a ordered dictionary to be able to print it nicely. The goal were to make it similar as PyTorch. 

\texttt{\_\_init\_\_}: Initialize the module layers parameter corresponding to the sub component inside a sequential model. 

\texttt{forward}: Compute the output of forward pass for each sub component using the precedent layer output. (Use the input of the first child.

\texttt{backward}: Similarly as forward pass, compute the backward pass for each sub component in reversed order using previous output as input for new child. Use the loss gradient at input for the last layer (which will be the first to call backward due to the reversed order). 

\textt{param}: Return the parameter and gradient pair for each sub component of the module. 

\texttt{zero\_grad}: Call zero\_grad function for each subcomponents.

\subsection{Mean squared error (MSE)}

The \texttt{MSE} module represents the mean squared error function and play the role of loss function to minimize in our problem. Mathematically, the MSE loss corresponds to:

\begin{equation}
    MSE = \frac{1}{n}\sum_{i=1}^{n} (y_i - pred_i)^2
\end{equation}

Where $y_i$ is the label i and $pred_i$ is the predication i.
The module store an error parameter being the difference between labels and prediction vectors.

\texttt{forward}: During the forward pass, the module compute the value of the loss of current prediction. It also stores the error (y - pred) into a variable for backward pass.

\texttt{backward}: The backward pass returns the derivative of the loss. 

\subsection{Optimizer}

We are now going to details optimizer component. Those are not neural network module but are still essential for the training. The optimizer define the policy that will update the weights in order to improve model performances. 

The optimizer abstract class goal is to define the main methods of an optimizer. 

\texttt{step}: The step function goal is to perform one unit of optimization for one model parameters. 


\subsection{Stochastic gradient descent (SGD)}

The \texttt{SGD} optimizer helps using stochastic gradient descent on model's loss (e.g. MSE). We algorithm use the following update rule to train the model:

\begin{equation}
    w = w - lr \times \nabla L
\end{equation}

With lr the learning rate (also called eta, or step), and $\nabla L$ the derivative of the loss function (returned in the loss backward pass). 

\texttt{\_\_init\_\_}: Initialize the learning rate and the model parameters (e.g. modules parameters and gradients).

\texttt{step}: Iterate over the model parameters, for each of them, update them with respect the learning rate and their gradients.

\texttt{set\_eta}: Update the learning rate value. This function can be used to build optimizer with adaptive learning rate.

\section{Usage}

We are going to describe here how a user could simply use our framework to build a feed-forward neural network and train it. 

\subsection{Model Definition}

In order to build a model, you can use multiple ways that are very similar to PyTorch library. You can either create an instance of sequential module and place each sub component you need in that module:

model = Sequential(OcteredDict( \\
                'linear 1': nn.Linear(2, 25), \\
                'relu 1': nn.Tanh(), \\
                'linear 2': nn.Linear(25, 25), \\
                'relu 2': nn.Tanh(), \\
                'linear 3': nn.Linear(25, 25), \\
                'relu 3': nn.Tanh(), \\
                'linear 4': nn.Linear(25, 2), \\
                'last act': nn.Tanh()))

As in PyTorch, you can also create a class inherited from the \texttt{neural\_nets.Module} class and that implement the necessary method. An example of this is shown in the \texttt{test.py} file.

\subsection{Model training}

In order to train your model, you first need to define the model as described above, as well as a criterion (the loss function) such as MSE and an optimizer such as SGD. Once this is done, you can iterate over your dataset and feed the model with the data using \texttt{model.forward()} and get a prediction. Using this prediction, you will be able to compute the loss using your criterion object and the data labels with \texttt{criterion.forward()}. This will give you an evaluation of the model current performance. Once you have it, you can perform the backward pass. You first need to make sure that the model has the gradients equal to zero (that it didn't keep the gradients for previous epoch or batch) using \texttt{model.zero\_grad()}. Finally, you can compute the gradient of the loss with \texttt{criterion.backward()}, use this gradient to update model parameters gradient using \texttt{model.backward()}. Once all this is done, the parameter can be updated with the newly computed gradients. You can therefore call the \texttt{optimizer.step()} to perform one step on the training. 

\subsection{Model testing}
In order to test the model, you simply need to used the \texttt{model.forward()} function with a training model on your test set to output model predictions. You can then compare the predictions to the test set labels.

\section{Results}

We provide a simple test of our framework to evaluate it's performances. The task consists of predicting if a given point (belonging to $[0, 1]^2$ space) is inside a circle of center $(0.5, 0.5)$ and or radius $1/\sqrt{2\pi}$. This test is performed in the \texttt{test.py} file. The \texttt{utils.py} file contains helper methods for data generation, train, and test methods. 

The network that make the prediction consists of 2 input units, 2 output unis, and 3 layers with 25 hidden units. The activation function used after each layer is \texttt{Tanh}. The model use the \texttt{MSE} loss and is optimized using \texttt{SGD} (eta=1e-3). 

We generated a train and test datasets and labels of 1000 samples each in order to train and test our model. The training were performed over 100 epochs and a batch size of 10. 

As the dataset is generated randomly, the model perform different accuracy for each trial. We run the full process for 20 iteration and got an median \textbf{test accuracy of 0.972}. This process is detailed and can be reproduced using \textt{test.ipynb}.


\end{document}
